{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2c4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adjust the path to point to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669b34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saurav\\OneDrive - URV\\Escritorio\\BfR\\DocParser\\ocr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import DocParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71fb84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocParser import DocParser, OCREngine\n",
    "from DocParser.Config.OCRConfig import OCRConfig\n",
    "from DocParser.Config.ModelConfig import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30b2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configurations\n",
    "config = OCRConfig(confidence_threshold=0.7,\n",
    "            preserve_layout=True,\n",
    "            extract_tables=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83093019",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "        device=\"cuda\",  # Use \"cuda\" if GPU is available\n",
    "        batch_size=3,\n",
    "        gpu_id=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8495e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/page_1.jpeg\"\n",
    "pdf_path = \"data/pdf1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337961f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DolphinEngine:Initializing Dolphin OCR engine with Hugging Face model\n",
      "INFO:DolphinEngine:Using CUDA device\n",
      "INFO:DocParser:Initialized dolphin OCR engine successfully\n",
      "INFO:DolphinEngine:Processing image: data/page_1.jpeg\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "INFO:DolphinEngine:Layout string: [0.28,0.09,0.72,0.10] title[PAIR_SEP][0.30,0.14,0.70,0.15] author[PAIR_SEP][0.28,0.15,0.72,0.20] para[PAIR_SEP][0.48,0.21,0.53,0.23] para[PAIR_SEP][0.33,0.26,0.39,0.27] sec[PAIR_SEP][0.25,0.28,0.47,0.45] para[PAIR_SEP][0.23,0.46,0.33,0.48] sec[PAIR_SEP][0.23,0.49,0.49,0.71] para[PAIR_SEP][0.23,0.72,0.49,0.88] para[RELATION_SEP][0.51,0.26,0.77,0.35] para[PAIR_SEP][0.51,0.37,0.77,0.59] para[PAIR_SEP][0.51,0.60,0.77,0.76] para[PAIR_SEP][0.51,0.78,0.77,0.92] para[PAIR_SEP][0.23,0.88,0.49,0.91] fnote[PAIR_SEP][0.25,0.91,0.45,0.92] fnote[PAIR_SEP][0.16,0.31,0.19,0.73] watermark\n",
      "INFO:DolphinEngine:Parsed layout regions: [([0.28, 0.09, 0.72, 0.1], 'title'), ([0.3, 0.14, 0.7, 0.15], 'author'), ([0.28, 0.15, 0.72, 0.2], 'para'), ([0.48, 0.21, 0.53, 0.23], 'para'), ([0.33, 0.26, 0.39, 0.27], 'sec'), ([0.25, 0.28, 0.47, 0.45], 'para'), ([0.23, 0.46, 0.33, 0.48], 'sec'), ([0.23, 0.49, 0.49, 0.71], 'para'), ([0.23, 0.72, 0.49, 0.88], 'para'), ([0.51, 0.26, 0.77, 0.35], 'para'), ([0.51, 0.37, 0.77, 0.59], 'para'), ([0.51, 0.6, 0.77, 0.76], 'para'), ([0.51, 0.78, 0.77, 0.92], 'para'), ([0.23, 0.88, 0.49, 0.91], 'fnote'), ([0.25, 0.91, 0.45, 0.92], 'fnote'), ([0.16, 0.31, 0.19, 0.73], 'watermark')]\n",
      "INFO:DolphinEngine:Processing region: title, bbox: [0.28, 0.09, 0.72, 0.1]\n",
      "INFO:DolphinEngine:Cropped region size: (275, 9)\n",
      "INFO:DolphinEngine:Content extracted: • Open and Efficient Foundation Language\n",
      "INFO:DolphinEngine:Processing region: author, bbox: [0.3, 0.14, 0.7, 0.15]\n",
      "INFO:DolphinEngine:Cropped region size: (251, 9)\n",
      "INFO:DolphinEngine:Content extracted: vron; Thibaut Lavril; Gautier Izacard; Xavier\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.28, 0.15, 0.72, 0.2]\n",
      "INFO:DolphinEngine:Cropped region size: (275, 45)\n",
      "INFO:DolphinEngine:Content extracted: _\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.48, 0.21, 0.53, 0.23]\n",
      "INFO:DolphinEngine:Cropped region size: (31, 18)\n",
      "INFO:DolphinEngine:Content extracted: Ieta A\n",
      "INFO:DolphinEngine:Processing region: sec, bbox: [0.33, 0.26, 0.39, 0.27]\n",
      "INFO:DolphinEngine:Cropped region size: (38, 9)\n",
      "INFO:DolphinEngine:Content extracted: _\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.25, 0.28, 0.47, 0.45]\n",
      "INFO:DolphinEngine:Cropped region size: (138, 153)\n",
      "INFO:DolphinEngine:Content extracted: LaMA, a collection of founda-\n",
      "nodels ranging from 7B to 65B\n",
      "e train our models on trillions\n",
      "show that it is possible to train\n",
      "models using publicly avail-\n",
      "exclusively, without resorting\n",
      "and inaccessible datasets. In\n",
      "MA-13B outperforms GPT-3\n",
      "st benchmarks, and LLaMA-\n",
      "titive with the best models,\n",
      "and PaLM-540B. We release\n",
      "o the research community $^1$ .\n",
      "INFO:DolphinEngine:Processing region: sec, bbox: [0.23, 0.46, 0.33, 0.48]\n",
      "INFO:DolphinEngine:Cropped region size: (63, 18)\n",
      "INFO:DolphinEngine:Content extracted: ion\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.23, 0.49, 0.49, 0.71]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 197)\n",
      "INFO:DolphinEngine:Content extracted: es Models (LLMs) trained on mas-\n",
      "texts have shown their ability to per-\n",
      "from textual instructions or from a\n",
      "Brown et al. , 2020 ) . These few-shot\n",
      "appeared when scaling models to a\n",
      "Kaplan et al. , 2020 ) , resulting in a\n",
      "at focuses on further scaling these\n",
      "lhery et al. , 2022 ; Rae et al. , 2021 ) .\n",
      "are based on the assumption that\n",
      "rs will lead to better performance.\n",
      "t work from Hoffmann et al. ( 2022 )\n",
      "a given compute budget, the best\n",
      "re not achieved by the largest mod-\n",
      "lier models trained on more data\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.23, 0.72, 0.49, 0.88]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 143)\n",
      "INFO:DolphinEngine:Content extracted: _\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.51, 0.26, 0.77, 0.35]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 81)\n",
      "INFO:DolphinEngine:Content extracted: performance, a smaller one train\n",
      "ultimately be cheaper at inference-\n",
      "although Hoffmann et al. ( 2022 )\n",
      "training a 10B model on 200B to\n",
      "that the performance of a 7B model\n",
      "improve even after 1T tokens.\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.51, 0.37, 0.77, 0.59]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 197)\n",
      "INFO:DolphinEngine:Content extracted: The focus of this work is to be\n",
      "language models that achieve the b-\n",
      "formance at various inference budi-\n",
      "on more tokens than what is typi-\n",
      "resulting models, called LLaMA ,\n",
      "to 65B parameters with competiti-\n",
      "compared to the best existing LLM-\n",
      "LLaMA-13B outperforms GPT-3\n",
      "marks, despite being 10 $\\times$ smaller. In\n",
      "this model will help democratize a\n",
      "study of LLMs, since it can be run on\n",
      "At the higher-end of the scale, our\n",
      "model is also competitive with the\n",
      "guage models such as Chinchilla et\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.51, 0.6, 0.77, 0.76]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 143)\n",
      "INFO:DolphinEngine:Content extracted: Unlike Chinchilla, PaLM, or C\n",
      "use publicly available data, making\n",
      "patible with open-sourcing, while\n",
      "models rely on data which is either\n",
      "available or undocumented (e.g. “ E-\n",
      "“ Social media conversations\"). The\n",
      "exceptions, notably OPT ( Zhang &\n",
      "GPT-NeoX ( Black et al. , 2022 ) , –\n",
      "et al. , 2022 ) and GLM ( Zeng et al. , 2018 )\n",
      "that are competitive with PaLM-67\n",
      "INFO:DolphinEngine:Processing region: para, bbox: [0.51, 0.78, 0.77, 0.92]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 126)\n",
      "INFO:DolphinEngine:Content extracted: In the rest of this paper, we pres-\n",
      "of the modifications we made to\n",
      "architecture ( Vaswani et al. , 2017 ) :\n",
      "training method. We then report the\n",
      "our models and compare with other\n",
      "of standard benchmarks. Finally, we\n",
      "of the biases and toxicity encoded\n",
      "using some of the most recent be-\n",
      "the responsible AI community.\n",
      "INFO:DolphinEngine:Processing region: fnote, bbox: [0.23, 0.88, 0.49, 0.91]\n",
      "INFO:DolphinEngine:Cropped region size: (163, 27)\n",
      "INFO:DolphinEngine:Content extracted: _\n",
      "INFO:DolphinEngine:Processing region: fnote, bbox: [0.25, 0.91, 0.45, 0.92]\n",
      "INFO:DolphinEngine:Cropped region size: (125, 9)\n",
      "INFO:DolphinEngine:Content extracted: com/facebookresearch/llama\n",
      "INFO:DolphinEngine:Processing region: watermark, bbox: [0.16, 0.31, 0.19, 0.73]\n",
      "INFO:DolphinEngine:Cropped region size: (18, 377)\n",
      "INFO:DolphinEngine:Content extracted: $^{2}$ Cd(p, $\\gamma$ ) $^{108}$ In reaction with prepared targets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with DocParser(OCREngine.DOLPHIN, config, model_config) as parser:\n",
    "            if os.path.exists(image_path):\n",
    "                result = parser.process_image(image_path)\n",
    "            else:\n",
    "                print(f\"   Image file not found: {image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9de0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Open and Efficient Foundation Language vron; Thibaut Lavril; Gautier Izacard; Xavier _ Ieta A _ LaMA, a collection of founda- nodels ranging from 7B to 65B e train our models on trillions show that it is possible to train models using publicly avail- exclusively, without resorting and inaccessible datasets. In MA-13B outperforms GPT-3 st benchmarks, and LLaMA- titive with the best models, and PaLM-540B. We release o the research community 1 . ion es Models (LLMs) trained on mas- texts have shown their ability to per- from textual instructions or from a Brown et al. , 2020 ) . These few-shot appeared when scaling models to a Kaplan et al. , 2020 ) , resulting in a at focuses on further scaling these lhery et al. , 2022 ; Rae et al. , 2021 ) . are based on the assumption that rs will lead to better performance. t work from Hoffmann et al. ( 2022 ) a given compute budget, the best re not achieved by the largest mod- lier models trained on more data _ performance, a smaller one train ultimately be cheaper at inference- although Hoffmann et al. ( 2022 ) training a 10B model on 200B to that the performance of a 7B model improve even after 1T tokens. The focus of this work is to be language models that achieve the b- formance at various inference budi- on more tokens than what is typi- resulting models, called LLaMA , to 65B parameters with competiti- compared to the best existing LLM- LLaMA-13B outperforms GPT-3 marks, despite being 10 times smaller. In this model will help democratize a study of LLMs, since it can be run on At the higher-end of the scale, our model is also competitive with the guage models such as Chinchilla et Unlike Chinchilla, PaLM, or C use publicly available data, making patible with open-sourcing, while models rely on data which is either available or undocumented (e.g.  E-  Social media conversations). The exceptions, notably OPT ( Zhang  GPT-NeoX ( Black et al. , 2022 ) ,  et al. , 2022 ) and GLM ( Zeng et al. , 2018 ) that are competitive with PaLM-67 In the rest of this paper, we pres- of the modifications we made to architecture ( Vaswani et al. , 2017 ) : training method. We then report the our models and compare with other of standard benchmarks. Finally, we of the biases and toxicity encoded using some of the most recent be- the responsible AI community. _ comfacebookresearchllama {2} Cd(p, gamma ) {108} In reaction with prepared targets\",\n",
      "  \"elements\": [\n",
      "    {\n",
      "      \"text\": \"Open and Efficient Foundation Language\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        175,\n",
      "        80,\n",
      "        450,\n",
      "        89\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"vron; Thibaut Lavril; Gautier Izacard; Xavier\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        187,\n",
      "        125,\n",
      "        438,\n",
      "        134\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"_\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        175,\n",
      "        134,\n",
      "        450,\n",
      "        179\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Ieta A\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        300,\n",
      "        188,\n",
      "        331,\n",
      "        206\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"_\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        206,\n",
      "        232,\n",
      "        244,\n",
      "        241\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"LaMA, a collection of founda- nodels ranging from 7B to 65B e train our models on trillions show that it is possible to train models using publicly avail- exclusively, without resorting and inaccessible datasets. In MA-13B outperforms GPT-3 st benchmarks, and LLaMA- titive with the best models, and PaLM-540B. We release o the research community 1 .\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        156,\n",
      "        250,\n",
      "        294,\n",
      "        403\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"ion\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        143,\n",
      "        412,\n",
      "        206,\n",
      "        430\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"es Models (LLMs) trained on mas- texts have shown their ability to per- from textual instructions or from a Brown et al. , 2020 ) . These few-shot appeared when scaling models to a Kaplan et al. , 2020 ) , resulting in a at focuses on further scaling these lhery et al. , 2022 ; Rae et al. , 2021 ) . are based on the assumption that rs will lead to better performance. t work from Hoffmann et al. ( 2022 ) a given compute budget, the best re not achieved by the largest mod- lier models trained on more data\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        143,\n",
      "        439,\n",
      "        306,\n",
      "        636\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"_\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        143,\n",
      "        645,\n",
      "        306,\n",
      "        788\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"performance, a smaller one train ultimately be cheaper at inference- although Hoffmann et al. ( 2022 ) training a 10B model on 200B to that the performance of a 7B model improve even after 1T tokens.\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        319,\n",
      "        232,\n",
      "        482,\n",
      "        313\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"The focus of this work is to be language models that achieve the b- formance at various inference budi- on more tokens than what is typi- resulting models, called LLaMA , to 65B parameters with competiti- compared to the best existing LLM- LLaMA-13B outperforms GPT-3 marks, despite being 10 times smaller. In this model will help democratize a study of LLMs, since it can be run on At the higher-end of the scale, our model is also competitive with the guage models such as Chinchilla et\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        319,\n",
      "        331,\n",
      "        482,\n",
      "        528\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Unlike Chinchilla, PaLM, or C use publicly available data, making patible with open-sourcing, while models rely on data which is either available or undocumented (e.g.  E-  Social media conversations). The exceptions, notably OPT ( Zhang  GPT-NeoX ( Black et al. , 2022 ) ,  et al. , 2022 ) and GLM ( Zeng et al. , 2018 ) that are competitive with PaLM-67\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        319,\n",
      "        537,\n",
      "        482,\n",
      "        680\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"In the rest of this paper, we pres- of the modifications we made to architecture ( Vaswani et al. , 2017 ) : training method. We then report the our models and compare with other of standard benchmarks. Finally, we of the biases and toxicity encoded using some of the most recent be- the responsible AI community.\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        319,\n",
      "        698,\n",
      "        482,\n",
      "        824\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"_\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        143,\n",
      "        788,\n",
      "        306,\n",
      "        815\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"comfacebookresearchllama\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        156,\n",
      "        815,\n",
      "        281,\n",
      "        824\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"{2} Cd(p, gamma ) {108} In reaction with prepared targets\",\n",
      "      \"confidence\": 0.92,\n",
      "      \"bbox\": [\n",
      "        100,\n",
      "        277,\n",
      "        118,\n",
      "        654\n",
      "      ],\n",
      "      \"element_type\": \"text\",\n",
      "      \"page_number\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"model_name\": \"Dolphin\",\n",
      "    \"source_file\": \"data/page_1.jpeg\"\n",
      "  },\n",
      "  \"processing_time\": 29.144527435302734,\n",
      "  \"model_name\": \"Dolphin\",\n",
      "  \"confidence_score\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "## Print the result in a readable format\n",
    "json_str = json.dumps(asdict(result), indent=2)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f52664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
