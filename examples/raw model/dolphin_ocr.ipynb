{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1588267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoProcessor, VisionEncoderDecoderModel\n",
    "from transformers.image_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "865414d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48b9e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =  \"ByteDance/Dolphin\"\n",
    "processor = AutoProcessor.from_pretrained(model_name,use_fast=True)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac93075",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if model.device.type == \"cuda\" else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9cf8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/table.jpg\"\n",
    "image = load_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4c58240",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "batch_pixel_values = batch_inputs.pixel_values.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "870a4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Parse the reading order of this document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6867243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f\"<s>{prompt} <Answer/>\"]\n",
    "tokenizer = processor.tokenizer\n",
    "prompt_inputs = tokenizer(prompts, add_special_tokens=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4682c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = prompt_inputs.input_ids.to(DEVICE)\n",
    "attention_mask = prompt_inputs.attention_mask.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c15bfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "     outputs = model.generate(\n",
    "                pixel_values=batch_pixel_values,\n",
    "                decoder_input_ids=prompt_ids,\n",
    "                decoder_attention_mask=attention_mask,\n",
    "                min_length=1,\n",
    "                max_length=4096,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                bad_words_ids=[[tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28a2f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = prompt_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3610cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_generated_ids = outputs.sequences[:, prompt_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af2a6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = processor.batch_decode(trimmed_generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcaf623b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [0.17,0.90,0.18,0.94] header[0.18,0.05,0.22,0.05] header[0.55,0.05,0.83,0.05] header[0.18,0.06,0.21,0.94] para[0.21,0.07,0.45,0.95] tab[0.52,0.07,0.83,0.10] anno[0.52,0.10,0.83,0.14] list[0.52,0.14,0.83,0.18] list[0.52,0.18,0.83,0.22] list[0.51,0.23,0.83,0.33] para[0.51,0.34,0.83,0.37] para[0.52,0.39,0.83,0.44] list[0.52,0.44,0.83,0.50] list[0.52,0.51,0.83,0.53] list[0.52,0.53,0.83,0.57] list[0.52,0.57,0.83,0.60] list[0.51,0.61,0.83,0.68] para[0.51,0.68,0.83,0.74] para[0.51,0.74,0.83,0.83] para[0.51,0.85,0.62,0.86] sub_sub_sec[0.51,0.86,0.83,0.94] para[0.49,0.96,0.51,0.96] foot']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee804f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, sequence in enumerate(seq):\n",
    "            cleaned = sequence.replace(prompts[i], \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            results.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab944e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_output = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "313234f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class ImageDimensions:\n",
    "    \"\"\"Class to store image dimensions\"\"\"\n",
    "\n",
    "    original_w: int\n",
    "    original_h: int\n",
    "    padded_w: int\n",
    "    padded_h: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bfc0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image):\n",
    "    \"\"\"Load and prepare image with padding while maintaining aspect ratio\n",
    "\n",
    "    Args:\n",
    "        image: PIL image\n",
    "\n",
    "    Returns:\n",
    "        tuple: (padded_image, image_dimensions)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert PIL image to OpenCV format\n",
    "        image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        original_h, original_w = image.shape[:2]\n",
    "\n",
    "        # Calculate padding to make square image\n",
    "        max_size = max(original_h, original_w)\n",
    "        top = (max_size - original_h) // 2\n",
    "        bottom = max_size - original_h - top\n",
    "        left = (max_size - original_w) // 2\n",
    "        right = max_size - original_w - left\n",
    "\n",
    "        # Apply padding\n",
    "        padded_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "        padded_h, padded_w = padded_image.shape[:2]\n",
    "\n",
    "        dimensions = ImageDimensions(original_w=original_w,\n",
    "                                     original_h=original_h,\n",
    "                                     padded_w=padded_w,\n",
    "                                     padded_h=padded_h)\n",
    "\n",
    "        return padded_image, dimensions\n",
    "    except Exception as e:\n",
    "        print(f\"prepare_image error: {str(e)}\")\n",
    "        # Create a minimal valid image and dimensions\n",
    "        h, w = image.height, image.width\n",
    "        dimensions = ImageDimensions(original_w=w, original_h=h, padded_w=w, padded_h=h)\n",
    "        # Return a black image of the same size\n",
    "        return np.zeros((h, w, 3), dtype=np.uint8), dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ff1d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparin the image\n",
    "padded_image, dims = prepare_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71d1004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_layout_string(bbox_str):\n",
    "    \"\"\"Parse layout string using regular expressions\"\"\"\n",
    "    pattern = r\"\\[(\\d*\\.?\\d+),\\s*(\\d*\\.?\\d+),\\s*(\\d*\\.?\\d+),\\s*(\\d*\\.?\\d+)\\]\\s*(\\w+)\"\n",
    "    matches = re.finditer(pattern, bbox_str)\n",
    "\n",
    "    parsed_results = []\n",
    "    for match in matches:\n",
    "        coords = [float(match.group(i)) for i in range(1, 5)]\n",
    "        label = match.group(5).strip()\n",
    "        parsed_results.append((coords, label))\n",
    "\n",
    "    return parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f3f7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import List\n",
    "def adjust_box_edges(image, boxes: List[List[float]], max_pixels=15, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Image: cv2.image object, or Path\n",
    "    Input: boxes: list of boxes [[x1, y1, x2, y2]]. Using absolute coordinates.\n",
    "    \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image)\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    new_boxes = []\n",
    "    for box in boxes:\n",
    "        best_box = copy.deepcopy(box)\n",
    "\n",
    "        def check_edge(img, current_box, i, is_vertical):\n",
    "            edge = current_box[i]\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "            if is_vertical:\n",
    "                line = binary[current_box[1] : current_box[3] + 1, edge]\n",
    "            else:\n",
    "                line = binary[edge, current_box[0] : current_box[2] + 1]\n",
    "\n",
    "            transitions = np.abs(np.diff(line))\n",
    "            return np.sum(transitions) / len(transitions)\n",
    "\n",
    "        # Only widen the box\n",
    "        edges = [(0, -1, True), (2, 1, True), (1, -1, False), (3, 1, False)]\n",
    "\n",
    "        current_box = copy.deepcopy(box)\n",
    "        # make sure the box is within the image\n",
    "        current_box[0] = min(max(current_box[0], 0), img_w - 1)\n",
    "        current_box[1] = min(max(current_box[1], 0), img_h - 1)\n",
    "        current_box[2] = min(max(current_box[2], 0), img_w - 1)\n",
    "        current_box[3] = min(max(current_box[3], 0), img_h - 1)\n",
    "\n",
    "        for i, direction, is_vertical in edges:\n",
    "            best_score = check_edge(image, current_box, i, is_vertical)\n",
    "            if best_score <= threshold:\n",
    "                continue\n",
    "            for step in range(max_pixels):\n",
    "                current_box[i] += direction\n",
    "                if i == 0 or i == 2:\n",
    "                    current_box[i] = min(max(current_box[i], 0), img_w - 1)\n",
    "                else:\n",
    "                    current_box[i] = min(max(current_box[i], 0), img_h - 1)\n",
    "                score = check_edge(image, current_box, i, is_vertical)\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_box = copy.deepcopy(current_box)\n",
    "\n",
    "                if score <= threshold:\n",
    "                    break\n",
    "        new_boxes.append(best_box)\n",
    "\n",
    "    return new_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2fef655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def map_to_original_coordinates(x1, y1, x2, y2, dims: ImageDimensions) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Map coordinates from padded image back to original image\n",
    "\n",
    "    Args:\n",
    "        x1, y1, x2, y2: Coordinates in padded image\n",
    "        dims: Image dimensions object\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x1, y1, x2, y2) coordinates in original image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate padding offsets\n",
    "        top = (dims.padded_h - dims.original_h) // 2\n",
    "        left = (dims.padded_w - dims.original_w) // 2\n",
    "\n",
    "        # Map back to original coordinates\n",
    "        orig_x1 = max(0, x1 - left)\n",
    "        orig_y1 = max(0, y1 - top)\n",
    "        orig_x2 = min(dims.original_w, x2 - left)\n",
    "        orig_y2 = min(dims.original_h, y2 - top)\n",
    "\n",
    "        # Ensure we have a valid box (width and height > 0)\n",
    "        if orig_x2 <= orig_x1:\n",
    "            orig_x2 = min(orig_x1 + 1, dims.original_w)\n",
    "        if orig_y2 <= orig_y1:\n",
    "            orig_y2 = min(orig_y1 + 1, dims.original_h)\n",
    "\n",
    "        return int(orig_x1), int(orig_y1), int(orig_x2), int(orig_y2)\n",
    "    except Exception as e:\n",
    "        print(f\"map_to_original_coordinates error: {str(e)}\")\n",
    "        # Return safe coordinates\n",
    "        return 0, 0, min(100, dims.original_w), min(100, dims.original_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "947dd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coordinates(coords, padded_image, dims: ImageDimensions, previous_box=None):\n",
    "    \"\"\"Process and adjust coordinates\n",
    "\n",
    "    Args:\n",
    "        coords: Normalized coordinates [x1, y1, x2, y2]\n",
    "        padded_image: Padded image\n",
    "        dims: Image dimensions object\n",
    "        previous_box: Previous box coordinates for overlap adjustment\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x1, y1, x2, y2, orig_x1, orig_y1, orig_x2, orig_y2, new_previous_box)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        x1, y1 = int(coords[0] * dims.padded_w), int(coords[1] * dims.padded_h)\n",
    "        x2, y2 = int(coords[2] * dims.padded_w), int(coords[3] * dims.padded_h)\n",
    "\n",
    "        # Ensure coordinates are within image bounds before adjustment\n",
    "        x1 = max(0, min(x1, dims.padded_w - 1))\n",
    "        y1 = max(0, min(y1, dims.padded_h - 1))\n",
    "        x2 = max(0, min(x2, dims.padded_w))\n",
    "        y2 = max(0, min(y2, dims.padded_h))\n",
    "\n",
    "        # Ensure width and height are at least 1 pixel\n",
    "        if x2 <= x1:\n",
    "            x2 = min(x1 + 1, dims.padded_w)\n",
    "        if y2 <= y1:\n",
    "            y2 = min(y1 + 1, dims.padded_h)\n",
    "\n",
    "        # Extend box boundaries\n",
    "        new_boxes = adjust_box_edges(padded_image, [[x1, y1, x2, y2]])\n",
    "        x1, y1, x2, y2 = new_boxes[0]\n",
    "\n",
    "        # Ensure coordinates are still within image bounds after adjustment\n",
    "        x1 = max(0, min(x1, dims.padded_w - 1))\n",
    "        y1 = max(0, min(y1, dims.padded_h - 1))\n",
    "        x2 = max(0, min(x2, dims.padded_w))\n",
    "        y2 = max(0, min(y2, dims.padded_h))\n",
    "\n",
    "        # Ensure width and height are at least 1 pixel after adjustment\n",
    "        if x2 <= x1:\n",
    "            x2 = min(x1 + 1, dims.padded_w)\n",
    "        if y2 <= y1:\n",
    "            y2 = min(y1 + 1, dims.padded_h)\n",
    "\n",
    "        # Check for overlap with previous box and adjust\n",
    "        if previous_box is not None:\n",
    "            prev_x1, prev_y1, prev_x2, prev_y2 = previous_box\n",
    "            if (x1 < prev_x2 and x2 > prev_x1) and (y1 < prev_y2 and y2 > prev_y1):\n",
    "                y1 = prev_y2\n",
    "                # Ensure y1 is still valid\n",
    "                y1 = min(y1, dims.padded_h - 1)\n",
    "                # Make sure y2 is still greater than y1\n",
    "                if y2 <= y1:\n",
    "                    y2 = min(y1 + 1, dims.padded_h)\n",
    "\n",
    "        # Update previous box\n",
    "        new_previous_box = [x1, y1, x2, y2]\n",
    "\n",
    "        # Map to original coordinates\n",
    "        orig_x1, orig_y1, orig_x2, orig_y2 = map_to_original_coordinates(x1, y1, x2, y2, dims)\n",
    "\n",
    "        return x1, y1, x2, y2, orig_x1, orig_y1, orig_x2, orig_y2, new_previous_box\n",
    "    except Exception as e:\n",
    "        print(f\"process_coordinates error: {str(e)}\")\n",
    "        # Return safe values\n",
    "        orig_x1, orig_y1, orig_x2, orig_y2 = 0, 0, min(100, dims.original_w), min(100, dims.original_h)\n",
    "        return 0, 0, 100, 100, orig_x1, orig_y1, orig_x2, orig_y2, [0, 0, 100, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a895ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_figure_to_local(pil_crop, save_dir, image_name, reading_order):\n",
    "    \"\"\"Save cropped figure to local file system\n",
    "\n",
    "    Args:\n",
    "        pil_crop: PIL Image object of the cropped figure\n",
    "        save_dir: Base directory to save results\n",
    "        image_name: Name of the source image/document\n",
    "        reading_order: Reading order of the figure in the document\n",
    "\n",
    "    Returns:\n",
    "        str: Filename of the saved figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create figures directory if it doesn't exist\n",
    "        figures_dir = os.path.join(save_dir, \"markdown\", \"figures\")\n",
    "        # os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "        # Generate figure filename\n",
    "        figure_filename = f\"{image_name}_figure_{reading_order:03d}.png\"\n",
    "        figure_path = os.path.join(figures_dir, figure_filename)\n",
    "\n",
    "        # Save the figure\n",
    "        pil_crop.save(figure_path, format=\"PNG\", quality=95)\n",
    "\n",
    "        # print(f\"Saved figure: {figure_filename}\")\n",
    "        return figure_filename\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving figure: {str(e)}\")\n",
    "        # Return a fallback filename\n",
    "        return f\"{image_name}_figure_{reading_order:03d}_error.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b49cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, image):\n",
    "    \"\"\"Process image(s) with the given prompt(s)\n",
    "    \n",
    "    Args:\n",
    "        prompt: String or list of prompts\n",
    "        image: PIL Image or list of PIL Images\n",
    "        \n",
    "    Returns:\n",
    "        List of generated text results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle single image/prompt case\n",
    "        is_single = not isinstance(image, list)\n",
    "        if is_single:\n",
    "            images = [image]\n",
    "            prompts = [prompt]\n",
    "        else:\n",
    "            images = image\n",
    "            prompts = prompt if isinstance(prompt, list) else [prompt] * len(images)\n",
    "            \n",
    "        # Process images\n",
    "        inputs = processor(\n",
    "            images=images,\n",
    "            text=prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device and handle precision\n",
    "        pixel_values = inputs.pixel_values.to(DEVICE)\n",
    "        if hasattr(model, 'dtype') and model.dtype == torch.float16:\n",
    "            pixel_values = pixel_values.half()\n",
    "            \n",
    "        # Prepare prompts\n",
    "        formatted_prompts = [f\"<s>{p} <Answer/>\" for p in prompts]\n",
    "        prompt_ids = processor.tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            add_special_tokens=False\n",
    "        ).input_ids.to(DEVICE)\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                decoder_input_ids=prompt_ids,\n",
    "                min_length=1,\n",
    "                max_length=4096,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                repetition_penalty=1.1,\n",
    "                temperature=1.0\n",
    "            )\n",
    "        \n",
    "        # Decode outputs\n",
    "        sequences = processor.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False)\n",
    "        \n",
    "        # Clean up sequences\n",
    "        results = []\n",
    "        for sequence, input_prompt in zip(sequences, formatted_prompts):\n",
    "            cleaned = sequence.replace(input_prompt, \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            results.append(cleaned)\n",
    "            \n",
    "        return results[0] if is_single else results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chat function: {str(e)}\")\n",
    "        if is_single:\n",
    "            return \"\"\n",
    "        return [\"\"] * len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "509b0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_element_batch(elements, model, prompt, max_batch_size):\n",
    "    \"\"\"Process elements of the same type in batches\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Use default batch size if None\n",
    "    batch_size = max_batch_size if max_batch_size else 4\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(elements), batch_size):\n",
    "        batch_elements = elements[i:i+batch_size]\n",
    "        crops_list = [elem[\"crop\"] for elem in batch_elements]\n",
    "        \n",
    "        # Process batch\n",
    "        batch_results = chat(prompt, crops_list)\n",
    "        \n",
    "        # Add results\n",
    "        for j, result in enumerate(batch_results):\n",
    "            elem = batch_elements[j]\n",
    "            results.append({\n",
    "                \"label\": elem[\"label\"],\n",
    "                \"bbox\": elem[\"bbox\"],\n",
    "                \"text\": result.strip(),\n",
    "                \"reading_order\": elem[\"reading_order\"],\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31c0418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_elements(layout_results, padded_image, dims, model, max_batch_size, save_dir=None, image_name=None):\n",
    "    \"\"\"Parse all document elements with parallel decoding\"\"\"\n",
    "    layout_results = parse_layout_string(layout_results)\n",
    "\n",
    "    # Store text and table elements separately\n",
    "    text_elements = []  # Text elements\n",
    "    table_elements = []  # Table elements\n",
    "    figure_results = []  # Image elements (no processing needed)\n",
    "    previous_box = None\n",
    "    reading_order = 0\n",
    "\n",
    "    # Collect elements to process and group by type\n",
    "    for bbox, label in layout_results:\n",
    "        try:\n",
    "            # Adjust coordinates\n",
    "            x1, y1, x2, y2, orig_x1, orig_y1, orig_x2, orig_y2, previous_box = process_coordinates(\n",
    "                bbox, padded_image, dims, previous_box\n",
    "            )\n",
    "\n",
    "            # Crop and parse element\n",
    "            cropped = padded_image[y1:y2, x1:x2]\n",
    "            if cropped.size > 0 and cropped.shape[0] > 3 and cropped.shape[1] > 3:\n",
    "                if label == \"fig\":\n",
    "                    pil_crop = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "                    \n",
    "                    figure_filename = save_figure_to_local(pil_crop, save_dir, image_name, reading_order)\n",
    "                    \n",
    "                    # For figure regions, store relative path instead of base64\n",
    "                    figure_results.append(\n",
    "                        {\n",
    "                            \"label\": label,\n",
    "                            \"text\": f\"![Figure](figures/{figure_filename})\",\n",
    "                            \"figure_path\": f\"figures/{figure_filename}\",\n",
    "                            \"bbox\": [orig_x1, orig_y1, orig_x2, orig_y2],\n",
    "                            \"reading_order\": reading_order,\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    # Prepare element for parsing\n",
    "                    pil_crop = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "                    element_info = {\n",
    "                        \"crop\": pil_crop,\n",
    "                        \"label\": label,\n",
    "                        \"bbox\": [orig_x1, orig_y1, orig_x2, orig_y2],\n",
    "                        \"reading_order\": reading_order,\n",
    "                    }\n",
    "                    \n",
    "                    # Group by type\n",
    "                    if label == \"tab\":\n",
    "                        table_elements.append(element_info)\n",
    "                    else:  # Text elements\n",
    "                        text_elements.append(element_info)\n",
    "\n",
    "            reading_order += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing bbox with label {label}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Initialize results list\n",
    "    recognition_results = figure_results.copy()\n",
    "    \n",
    "    # Process text elements (in batches)\n",
    "    if text_elements:\n",
    "        text_results = process_element_batch(text_elements, model, \"Read text in the image.\", max_batch_size)\n",
    "        recognition_results.extend(text_results)\n",
    "    \n",
    "    # Process table elements (in batches)\n",
    "    if table_elements:\n",
    "        table_results = process_element_batch(table_elements, model, \"Parse the table in the image.\", max_batch_size)\n",
    "        recognition_results.extend(table_results)\n",
    "\n",
    "    # Sort elements by reading order\n",
    "    recognition_results.sort(key=lambda x: x.get(\"reading_order\", 0))\n",
    "\n",
    "    return recognition_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30f922db",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_results = process_elements(layout_output, padded_image, dims, model, None, \"data\", \"table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02fe3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme_from_results(recognition_results):\n",
    "    \"\"\"Generate a structured README from OCR recognition results\n",
    "    \n",
    "    Args:\n",
    "        recognition_results: List of dictionaries containing OCR results with labels and text\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted README content\n",
    "    \"\"\"\n",
    "    # Initialize sections\n",
    "    title = \"\"\n",
    "    headers = []\n",
    "    paragraphs = []\n",
    "    lists = []\n",
    "    tables = []\n",
    "    current_section = \"\"\n",
    "    \n",
    "    # Process results in reading order\n",
    "    for item in recognition_results:\n",
    "        label = item['label']\n",
    "        text = item['text'].strip()\n",
    "        \n",
    "        # Skip empty text\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # Process by label type\n",
    "        if label == 'header':\n",
    "            headers.append(text)\n",
    "            if not title and len(text) > 10:  # Assume longer header is title\n",
    "                title = text\n",
    "                \n",
    "        elif label == 'para':\n",
    "            paragraphs.append(text)\n",
    "            \n",
    "        elif label == 'list':\n",
    "            lists.append(text)\n",
    "            \n",
    "        elif label == 'tab':\n",
    "            tables.append(text)\n",
    "    \n",
    "    # Build README content\n",
    "    readme = []\n",
    "    \n",
    "    # Add title\n",
    "    if title:\n",
    "        readme.append(f\"# {title}\\n\")\n",
    "    \n",
    "    # Add remaining headers as sections\n",
    "    for header in headers:\n",
    "        if header != title:\n",
    "            readme.append(f\"## {header}\\n\")\n",
    "    \n",
    "    # Add paragraphs\n",
    "    if paragraphs:\n",
    "        readme.append(\"## Description\\n\")\n",
    "        for para in paragraphs:\n",
    "            readme.append(f\"{para}\\n\\n\")\n",
    "    \n",
    "    # Add lists\n",
    "    if lists:\n",
    "        readme.append(\"## Key Points\\n\")\n",
    "        for item in lists:\n",
    "            readme.append(f\"- {item}\\n\")\n",
    "        readme.append(\"\\n\")\n",
    "    \n",
    "    # Add tables \n",
    "    if tables:\n",
    "        readme.append(\"## Tables\\n\")\n",
    "        for table in tables:\n",
    "            readme.append(f\"{table}\\n\\n\")\n",
    "    \n",
    "    return \"\".join(readme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "993bb7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Crépet et a\n",
      "## Table 1\n",
      "## International Journal of Hygiene and Environmental Health 222 (2019) 291–306\n",
      "## Description\n",
      "Table 1\n",
      "Description of consumption and concentration data for nine different European countries, n total – number of individuals in the overall consumption survey, n – number of individuals included in this study\n",
      "(adults 18-64 years old, children 11-15 years old), N – number of substances in steatosis CAG after matching with contamination date; the number in brackets indicates the number of substances with\n",
      "\n",
      "Substances were coded using the ParamCodes from the harmonised\n",
      "European Standard Sample Description 1 format SDD1 ( EFSA, 2010 ) .\n",
      "Substances were removed if no ParamCode coding for pesticides, no\n",
      "NOAL or no LOAEL (copper compounds) were available. Some sub-\n",
      "stances sharing the same residue definition (benalaxyl-M and benalaxyl,\n",
      "cypermethrin and alpha-cypermethrin, metam and dazomet, metaloxyl-\n",
      "M and metalaxyl, triadimefon and triadimenol) were presented together\n",
      "in the database. This approach resulted in a total of 144 pesticides.\n",
      "\n",
      "Relative potency factors (RPFs) were calculated to express the po-\n",
      "tency of each substance in the CAG relative to a selected reference\n",
      "compound chosen based on the following criteria:\n",
      "\n",
      "The minimum required data set for calculation of potency was a\n",
      "well-performed chronic study with a dose-range that could provide a\n",
      "LOAEL for steatosis. The more studies available, the extent to which the\n",
      "above mentioned criteria could be applied to select the NOAEL or\n",
      "LOAEL of a particular substance to calculate its potency.\n",
      "\n",
      "Flusilazole complying with the above criteria was selected as the\n",
      "reference compound. Data came from 4 long term studies where liver\n",
      "effects were evident and LOAEL/NOAEL ratio for fatty changes spaced\n",
      "between 2 and 5. Its NOAEL for fatty changes was of 0.53 mg/kg bw/\n",
      "day.\n",
      "\n",
      "For each compound, the NOAEL of flusilazole was divided by the\n",
      "NOAEL of the particular compound, which yielded the RPF. When no\n",
      "NOAEL was available, the LOAEL divided by three was used as an as-\n",
      "sumption of the NOAEL. The RPFs make it possible to convert exposure\n",
      "to the substances into the “toxicity unit” of the reference compound,\n",
      "and thus to compare the exposure levels between substances within a\n",
      "CAG.\n",
      "\n",
      "Food consumption data from the different countries were coded\n",
      "according to the harmonised FoodEx1 coding system ( EFSA , 2011 ) .\n",
      "FoodEx1 is a hierarchical system based on 20 main food categories\n",
      "divided into subgroups up to a maximum of 4 levels. For example,\n",
      "chocolate cake is given a numerical code responding to `grain and\n",
      "grain-based products' at level 1, to `fine bakery wares' at level 2, to\n",
      "\n",
      "## Key Points\n",
      "- • Studies performed with metabolites were not included, except when\n",
      "the metabolite itself was used in the toxicity studies instead of the\n",
      "parent compound due to its high instability.\n",
      "- • In the particular case where the active substance consists of isomer\n",
      "mixtures, the studies performed with the racemic mixture and those\n",
      "carried out with the different isomers were reported.\n",
      "• When different isomers and/or variants were considered to be tox-\n",
      "- icologically equivalent, the same specific effect was applied and the\n",
      "studies were reported only once.\n",
      "- • Considering that longer-term studies (i.e.12, 18 and 24 months)\n",
      "were generally performed using lower concentrations compared to\n",
      "shorter-term studies (i.e. 28 or 90 days), priority was given to long-\n",
      "term studies.\n",
      "- • Compounds characterised by an NOAEL causing fatty changes\n",
      "(steatosis) between 0 and 1 mg/kg bw/day, were first selected (to\n",
      "avoid the selection of an index compound eliciting other organ and/\n",
      "or different liver effects at doses lower that those eliciting fatty\n",
      "changes).\n",
      "- • The second step in selection was made on the basis of the LOAEL/\n",
      "NOAEL ratio (between 1 and 5) to avoid dose-spacing uncertainties.\n",
      "- • The third step in selection was made taking into consideration only\n",
      "those compounds also causing cell degeneration/cell alteration or\n",
      "cell death at similar or higher doses.\n",
      "- • As a final step, the compound with more studies showing liver ef-\n",
      "fects was chosen as the reference compound.\n",
      "\n",
      "## Tables\n",
      "<table><tr><td></td><td>1st Degree of Freedom</td><td>2nd Degree of Freedom</td><td>3rd Degree of Freedom</td></tr><tr><td>Distance to nearest neighbor(A)</td><td>0.986745</td><td>0.986745</td><td>0.986745</td></tr></table>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_readme_from_results(recognition_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689461f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
